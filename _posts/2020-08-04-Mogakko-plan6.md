---
layout: post
title: "모각코 6회차"
---
### 목표 : 정확도가 향상된 이미지 분류기 만들기

- - -
#### -여러 방법으로 신경망을 정의하기
###### numpy 사용, nn패키지 사용, optim패키지 사용 등 좋은 신경망을 구성하기 위해 여러 가지 코드를 보고 실습해보았다. 그 중 최적화 알고리즘을 SGD에서 Adam으로 바꾸면 정확도가 상승할 것이라 생각해 Adam으로 학습시킨 후 정확도를 측정하였다. Adam이 SGD보다 좀 더 정교한 최적화 알고리즘으로 알려져 있어 정확도 향상을 기대했는데 오히려 정확도가 50%로 떨어졌다. 내가 만든 신경망의 층이 너무 적어서 오히려 정교하게 최적화하는 방법이 악영향을 미친 건가 추측한다. 자세한 이유는 아직 찾지 못했다. 이것저것 건드려보다가 마지막에 학습률을 변화시켰는데 정확도가 향상됐다. 0.001에서 0.01로 바꿨더니 정확도가 64%가 나왔고 지난주보다 6% 향상되었다.

#### -학습률
###### 학습률(Learning Rate)은 한 번 학습할 때 학습해야 할 학습량을 의미하며 학습한 이후에 한 번의 학습량으로 가중치 매개변수가 갱신된다. 
###### 학습률은 하이퍼파라미터(hyperparameter)라고 부르며 가중치와 편향 같은 신경망의 매개변수와는 다르다. 가중치 등은 훈련용 데이터 셋과 학습 알고리즘에 따라 자동적으로 생성되지만 학습률 같은 하이퍼파라미터는 사람이 수동적으로 설정해야 하기 때문에 시험을 통해서 가장 적절한 값을 찾아가야 한다.
###### 학습률 값은 미리 0.01, 0.001과 같이 특정 값을 정해두어야 하며 일반적으로 이 값이 너무 크거나 작으면 적합한 지점으로 찾아가기가 어렵다. 신경망 학습에서는 보통 이 학습률 값을 변경하면서 올바르게 학습하고 있는지를 확인한다.

#### -학습내용
###### Jupyter Notebook으로 학습한 파일 '_Project_Mogakko'에 '신경망 관련 추가학습','정확도향상시키기'로 올려놓았다.
[Github Link](https://github.com/OMEGA-Y/OMEGA-Y.github.io)
- - -

### 결과
###### 정확도를 향상시키기 위해 신경망을 여러 가지 방법으로 정의하는 것을 시도했다. 우선, 여러 방법(numpy사용, nn패키지 사용, optim패키지 사용 등)으로 신경망을 만드는 방법을 학습하며 어떤 요소를 변화시켜야 정확도를 향상시킬 수 있을지 생각했다. 최적화 알고리즘 바꾸기, dropout 등 이것저것 시도를 해보았는데, 정확도가 오히려 떨어지는 결과가 발생했다. 마지막에, 학습률을 여러 값으로 변화시켜봄으로써 정확도가 향상되는 결과를 얻을 수 있었다. GPU가 아닌 CPU를 이용하다 보니 시도할 수 있는 방법에 제약이 많았던 점이 아쉬웠다.
### 마무리하며....
###### 사실 6주동안 딥러닝에 대해 많은 것을 학습하고 싶었지만 쉽지는 않았다. 프로그래밍 언어 자체를 다룬지도 얼마 되지 않다보니 첫주차에 세팅부터 굉장히 해맸고, 언어에 대한 기본 개념을 학습하는 것도 이해하기 어려운 부분이 많았다. 그래도 정해진 시간에 학습을 해야하는 강제성 덕분에 꾸준히 했더니 모각코 전보다는 더 나아졌다고 느낀다!