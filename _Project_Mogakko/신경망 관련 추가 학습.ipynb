{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 관련 추가 학습\n",
    "(참고 : 파이토치튜토리얼 - 예제로 배우는 파이토치)\n",
    "\n",
    "ReLU 신경망을 예제로 사용한다. 이 신경망은 하나의 은닉층 (hidden layer)을 갖고 있으며, 신경망의 출력과 정답 사이의 유클리드 거리 (Euclidean distance)를 최소화하는 식으로 경사하강법(gradient descent)을 사용하여 무작위의 데이터를 맞추도록 학습한다. \n",
    "\n",
    "여러 가지 패키지를 이용해 다양한 방법으로 신경망을 구성해보겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy를 사용한 신경망을 구성해보자.\n",
    "\n",
    "NumPy는 N차원 배열 객체과 같은 배열들을 조작하기 위한 다양한 함수를 제공한다. NumPy 연산을 사용하여 순전파 단계와 역전파 단계를 직접 구현함으로써, 2계층(two-layer)을 갖는 신경망이 무작위의 데이터를 맞추도록 할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 33161658.239218708\n",
      "1 27195940.433474064\n",
      "2 24332029.525697783\n",
      "3 20841073.983486116\n",
      "4 16175364.396635894\n",
      "5 11178939.188116077\n",
      "6 7141376.577036583\n",
      "7 4409297.190405185\n",
      "8 2777646.6525287125\n",
      "9 1837684.8475821982\n",
      "10 1295548.895976421\n",
      "11 968909.6994100075\n",
      "12 760207.4930175476\n",
      "13 617747.3716856532\n",
      "14 514592.63701045635\n",
      "15 436042.11928309646\n",
      "16 373953.74092161347\n",
      "17 323502.5410496566\n",
      "18 281796.40595202695\n",
      "19 246828.0175246825\n",
      "20 217166.81248066502\n",
      "21 191837.0827962893\n",
      "22 170104.1383574393\n",
      "23 151304.2463714442\n",
      "24 134951.23337602918\n",
      "25 120670.61042552782\n",
      "26 108179.86522879527\n",
      "27 97192.22983303432\n",
      "28 87498.20520992175\n",
      "29 78926.33207727775\n",
      "30 71323.35018585651\n",
      "31 64564.29044866665\n",
      "32 58538.73716074727\n",
      "33 53157.52899543967\n",
      "34 48339.22870816703\n",
      "35 44015.24819262219\n",
      "36 40126.519264089104\n",
      "37 36623.3619332516\n",
      "38 33465.52410965476\n",
      "39 30614.086451309868\n",
      "40 28033.048837121933\n",
      "41 25696.117279584127\n",
      "42 23575.771418339977\n",
      "43 21648.804623230375\n",
      "44 19895.735346577138\n",
      "45 18299.298844652032\n",
      "46 16843.214340894836\n",
      "47 15514.791143264192\n",
      "48 14304.312724029849\n",
      "49 13197.031620566282\n",
      "50 12183.319166775818\n",
      "51 11254.384716481316\n",
      "52 10402.530329748348\n",
      "53 9620.413873157726\n",
      "54 8901.957368460624\n",
      "55 8241.803456154921\n",
      "56 7634.635382948456\n",
      "57 7075.381461360268\n",
      "58 6560.204243594154\n",
      "59 6085.354810174145\n",
      "60 5647.490705802889\n",
      "61 5243.463525118226\n",
      "62 4870.272434026753\n",
      "63 4525.423102627135\n",
      "64 4206.860657742368\n",
      "65 3912.1610563751337\n",
      "66 3639.497796687508\n",
      "67 3387.09711469607\n",
      "68 3153.3897048854064\n",
      "69 2936.6949225937906\n",
      "70 2735.785947265181\n",
      "71 2549.481957652792\n",
      "72 2376.589047892588\n",
      "73 2216.1707521170647\n",
      "74 2067.259273546095\n",
      "75 1928.8844685809302\n",
      "76 1800.2769821993043\n",
      "77 1680.721869305126\n",
      "78 1569.4898300630102\n",
      "79 1465.9943947914062\n",
      "80 1369.7836175680402\n",
      "81 1280.30523367453\n",
      "82 1196.9339267260602\n",
      "83 1119.3094714520914\n",
      "84 1047.0505002837422\n",
      "85 979.6841056053511\n",
      "86 916.9005100410868\n",
      "87 858.3234564989133\n",
      "88 803.6635872863266\n",
      "89 752.650709504478\n",
      "90 705.0152901627249\n",
      "91 660.5310817967088\n",
      "92 618.9535909377381\n",
      "93 580.1074121810661\n",
      "94 543.8101031834057\n",
      "95 509.8918571161629\n",
      "96 478.1602466005741\n",
      "97 448.4827418577565\n",
      "98 420.72167333239446\n",
      "99 394.7582349690415\n",
      "100 370.4539640216044\n",
      "101 347.6998237514458\n",
      "102 326.40632202208707\n",
      "103 306.4594048559345\n",
      "104 287.7769839547205\n",
      "105 270.2763005497323\n",
      "106 253.87451071460055\n",
      "107 238.5000842356363\n",
      "108 224.08613871129353\n",
      "109 210.5718808983062\n",
      "110 197.90460586770698\n",
      "111 186.01993982171194\n",
      "112 174.87160446341994\n",
      "113 164.41295199374852\n",
      "114 154.59829691822367\n",
      "115 145.3890915217174\n",
      "116 136.74392476129051\n",
      "117 128.6324141516123\n",
      "118 121.01158797980321\n",
      "119 113.85566153881268\n",
      "120 107.13732870812537\n",
      "121 100.82475260518264\n",
      "122 94.89474627208446\n",
      "123 89.3224267908375\n",
      "124 84.08857505748811\n",
      "125 79.16898171025304\n",
      "126 74.54397883633811\n",
      "127 70.19601509889395\n",
      "128 66.1086246906512\n",
      "129 62.26418527244671\n",
      "130 58.649797772654765\n",
      "131 55.250517151015224\n",
      "132 52.053517598956844\n",
      "133 49.04450511260618\n",
      "134 46.21445693440752\n",
      "135 43.55344590460893\n",
      "136 41.04727917512351\n",
      "137 38.687872950219514\n",
      "138 36.46765628192358\n",
      "139 34.37831471172986\n",
      "140 32.41163593849542\n",
      "141 30.559123157774945\n",
      "142 28.814989794559487\n",
      "143 27.172164618346777\n",
      "144 25.624749073275662\n",
      "145 24.167435628992003\n",
      "146 22.79444227504049\n",
      "147 21.5018319864864\n",
      "148 20.284374987270102\n",
      "149 19.13660056014527\n",
      "150 18.054441128655398\n",
      "151 17.0347319771874\n",
      "152 16.074019858382993\n",
      "153 15.168286964516097\n",
      "154 14.31457449352645\n",
      "155 13.509957536327068\n",
      "156 12.75112240624532\n",
      "157 12.035750701536323\n",
      "158 11.361306642194135\n",
      "159 10.725115863958337\n",
      "160 10.125134381830776\n",
      "161 9.559590282814074\n",
      "162 9.026407368454185\n",
      "163 8.52296733204392\n",
      "164 8.048276315668138\n",
      "165 7.6003254915310325\n",
      "166 7.177610458667645\n",
      "167 6.7788597234139125\n",
      "168 6.402609729685301\n",
      "169 6.0477406735457055\n",
      "170 5.712783475903205\n",
      "171 5.396501979869765\n",
      "172 5.097966943226714\n",
      "173 4.816249914708788\n",
      "174 4.55035867115553\n",
      "175 4.299507814983956\n",
      "176 4.0625432385566445\n",
      "177 3.838823828558579\n",
      "178 3.6275405863486854\n",
      "179 3.4280774650991552\n",
      "180 3.2398001145953295\n",
      "181 3.0621343094724613\n",
      "182 2.8943685874161416\n",
      "183 2.7358765473712183\n",
      "184 2.586272463119547\n",
      "185 2.4448766558791144\n",
      "186 2.311306229436959\n",
      "187 2.1851470957522725\n",
      "188 2.0660375000490387\n",
      "189 1.9534193711689274\n",
      "190 1.8470233934265345\n",
      "191 1.7465143696607446\n",
      "192 1.6515240241696627\n",
      "193 1.5617586040319753\n",
      "194 1.4769324186112387\n",
      "195 1.3967685877961262\n",
      "196 1.3210374321729008\n",
      "197 1.2494482071283337\n",
      "198 1.1817851336855878\n",
      "199 1.1178303830277423\n",
      "200 1.0573684314486895\n",
      "201 1.0002206263687163\n",
      "202 0.9461754246666855\n",
      "203 0.8950812070194024\n",
      "204 0.846768252018364\n",
      "205 0.8011110453798334\n",
      "206 0.7579376505705233\n",
      "207 0.7171183212702921\n",
      "208 0.678509381772566\n",
      "209 0.6419940129137494\n",
      "210 0.6074723284148146\n",
      "211 0.5748273603881051\n",
      "212 0.5440033650027891\n",
      "213 0.5148637225535317\n",
      "214 0.48729475400636335\n",
      "215 0.4612133292455556\n",
      "216 0.436540832020721\n",
      "217 0.4132019465281698\n",
      "218 0.39112747228001377\n",
      "219 0.37024674513319733\n",
      "220 0.3504902831278161\n",
      "221 0.33179420627492806\n",
      "222 0.31410580970844243\n",
      "223 0.2973676994752602\n",
      "224 0.28154050995502244\n",
      "225 0.2665609102943091\n",
      "226 0.25239225182396785\n",
      "227 0.2389733460029256\n",
      "228 0.22627424071900804\n",
      "229 0.21425507973829644\n",
      "230 0.2028826419226149\n",
      "231 0.19211488058294623\n",
      "232 0.18192393525919248\n",
      "233 0.17228157295619223\n",
      "234 0.16315069939341528\n",
      "235 0.15450922482312493\n",
      "236 0.14633192254629365\n",
      "237 0.13858974483974185\n",
      "238 0.13126266903508033\n",
      "239 0.12432366357639357\n",
      "240 0.11775514632767659\n",
      "241 0.11153510580284541\n",
      "242 0.10564635367284034\n",
      "243 0.10007169031225316\n",
      "244 0.09479295379558653\n",
      "245 0.0897952697172025\n",
      "246 0.08506258474137104\n",
      "247 0.08058058246902747\n",
      "248 0.07633692630064833\n",
      "249 0.07231908599983444\n",
      "250 0.06851603891910762\n",
      "251 0.06491290057941781\n",
      "252 0.0615002278399448\n",
      "253 0.058268431537297466\n",
      "254 0.0552071864263582\n",
      "255 0.0523085222736707\n",
      "256 0.04956230835256559\n",
      "257 0.04696158289778215\n",
      "258 0.04450034181277175\n",
      "259 0.04216816172168051\n",
      "260 0.039957825889858564\n",
      "261 0.0378655544064986\n",
      "262 0.03588294631532639\n",
      "263 0.03400416217924238\n",
      "264 0.0322243186122783\n",
      "265 0.03053896338053902\n",
      "266 0.028942320239605895\n",
      "267 0.027429315087692887\n",
      "268 0.025996180630298182\n",
      "269 0.024638011301399452\n",
      "270 0.023351318986409436\n",
      "271 0.02213218294101176\n",
      "272 0.020977573807083136\n",
      "273 0.01988428364805489\n",
      "274 0.01884738582001926\n",
      "275 0.01786496241872855\n",
      "276 0.01693386583928745\n",
      "277 0.016051799824881183\n",
      "278 0.015215858226631154\n",
      "279 0.014423907074350335\n",
      "280 0.013673492791479863\n",
      "281 0.012962230209894267\n",
      "282 0.012288000133036568\n",
      "283 0.011649040147011371\n",
      "284 0.01104388667450952\n",
      "285 0.010470254939226962\n",
      "286 0.00992661365439989\n",
      "287 0.009411260353365853\n",
      "288 0.008922703491567605\n",
      "289 0.008459776102649428\n",
      "290 0.00802120058877576\n",
      "291 0.007605260709646332\n",
      "292 0.007211018852206514\n",
      "293 0.006837444540670408\n",
      "294 0.006483241450659368\n",
      "295 0.00614751766503981\n",
      "296 0.0058291331312460065\n",
      "297 0.005527494581783282\n",
      "298 0.00524144899417505\n",
      "299 0.00497039542055794\n",
      "300 0.004713332339222743\n",
      "301 0.004469667954281152\n",
      "302 0.00423860488755219\n",
      "303 0.004019584118668609\n",
      "304 0.00381197022752672\n",
      "305 0.00361521932034389\n",
      "306 0.0034286256627484198\n",
      "307 0.0032516437142782336\n",
      "308 0.003083878051505562\n",
      "309 0.0029247911206100666\n",
      "310 0.002773990414780223\n",
      "311 0.00263096546054359\n",
      "312 0.0024953686141504757\n",
      "313 0.0023667985826280238\n",
      "314 0.002244871854247986\n",
      "315 0.0021292703881792604\n",
      "316 0.00201969313308375\n",
      "317 0.0019157471127741238\n",
      "318 0.0018171682078298476\n",
      "319 0.001723689292538817\n",
      "320 0.0016350356623857575\n",
      "321 0.0015509877056695649\n",
      "322 0.0014713354891174436\n",
      "323 0.00139572217436534\n",
      "324 0.0013240262711402924\n",
      "325 0.0012560151089339349\n",
      "326 0.0011915364610192753\n",
      "327 0.001130368095747885\n",
      "328 0.0010723712682893741\n",
      "329 0.001017341092801861\n",
      "330 0.0009651478868904603\n",
      "331 0.0009156495977104512\n",
      "332 0.0008686920858743405\n",
      "333 0.000824172388240613\n",
      "334 0.0007819377990332976\n",
      "335 0.0007418798418801621\n",
      "336 0.0007038944326038872\n",
      "337 0.0006678444606345112\n",
      "338 0.0006336481143181277\n",
      "339 0.00060121645751341\n",
      "340 0.0005704556442675205\n",
      "341 0.0005412680466437315\n",
      "342 0.0005135808954298631\n",
      "343 0.000487320626827963\n",
      "344 0.0004624044718536363\n",
      "345 0.0004387645252216778\n",
      "346 0.0004163550551576356\n",
      "347 0.00039508212579453554\n",
      "348 0.00037490508862446213\n",
      "349 0.00035576360924432127\n",
      "350 0.00033759837537083927\n",
      "351 0.0003203638560144311\n",
      "352 0.0003040202383964783\n",
      "353 0.00028851560902659596\n",
      "354 0.000273799528928846\n",
      "355 0.0002598364517727639\n",
      "356 0.000246590662029562\n",
      "357 0.00023402006365044517\n",
      "358 0.0002220905246229791\n",
      "359 0.00021077475817964683\n",
      "360 0.00020003886187596874\n",
      "361 0.00018985114815149627\n",
      "362 0.00018018635747806645\n",
      "363 0.00017101325035641908\n",
      "364 0.0001623073552468674\n",
      "365 0.0001540478830477178\n",
      "366 0.00014621131415759164\n",
      "367 0.00013877409002971517\n",
      "368 0.0001317175252491992\n",
      "369 0.00012501994980286532\n",
      "370 0.00011866372667409844\n",
      "371 0.00011263129117331548\n",
      "372 0.00010690742225303917\n",
      "373 0.00010147697276194382\n",
      "374 9.632474153781567e-05\n",
      "375 9.143353471807611e-05\n",
      "376 8.679047629572367e-05\n",
      "377 8.238350567165642e-05\n",
      "378 7.820139041396295e-05\n",
      "379 7.42347102468013e-05\n",
      "380 7.046959343334072e-05\n",
      "381 6.689605337339088e-05\n",
      "382 6.350482543476894e-05\n",
      "383 6.02846477092524e-05\n",
      "384 5.722826935441662e-05\n",
      "385 5.4328147358989186e-05\n",
      "386 5.157570977241285e-05\n",
      "387 4.8962772632910216e-05\n",
      "388 4.648242252385315e-05\n",
      "389 4.41281306129202e-05\n",
      "390 4.1893309519671104e-05\n",
      "391 3.977278554214607e-05\n",
      "392 3.77597385655162e-05\n",
      "393 3.5849627095138555e-05\n",
      "394 3.403518283140664e-05\n",
      "395 3.2313094035045606e-05\n",
      "396 3.06785547422118e-05\n",
      "397 2.9127219133180745e-05\n",
      "398 2.765449383104992e-05\n",
      "399 2.625707758660617e-05\n",
      "400 2.4929962899516386e-05\n",
      "401 2.367025254039354e-05\n",
      "402 2.2474338376527975e-05\n",
      "403 2.1339018563218637e-05\n",
      "404 2.026114900563335e-05\n",
      "405 1.923818754158942e-05\n",
      "406 1.8266956289169817e-05\n",
      "407 1.734496431766275e-05\n",
      "408 1.646938086559785e-05\n",
      "409 1.563858302148917e-05\n",
      "410 1.4850061846298745e-05\n",
      "411 1.4101045781172249e-05\n",
      "412 1.3389862979353893e-05\n",
      "413 1.2714674778393301e-05\n",
      "414 1.2073553298526954e-05\n",
      "415 1.1465041451915278e-05\n",
      "416 1.0887174218943768e-05\n",
      "417 1.0338667611317754e-05\n",
      "418 9.817834382141288e-06\n",
      "419 9.323346550483406e-06\n",
      "420 8.853713946606975e-06\n",
      "421 8.407860130561955e-06\n",
      "422 7.984487899620124e-06\n",
      "423 7.582598938180838e-06\n",
      "424 7.200898265651476e-06\n",
      "425 6.83850070013787e-06\n",
      "426 6.494484060203986e-06\n",
      "427 6.1678217097904905e-06\n",
      "428 5.857579702463664e-06\n",
      "429 5.563012736861954e-06\n",
      "430 5.283296217384197e-06\n",
      "431 5.017619666804025e-06\n",
      "432 4.765375758233334e-06\n",
      "433 4.525985402754581e-06\n",
      "434 4.298587485270274e-06\n",
      "435 4.082694800395953e-06\n",
      "436 3.877743829629563e-06\n",
      "437 3.6829306724773008e-06\n",
      "438 3.4979891116819032e-06\n",
      "439 3.3223733103737348e-06\n",
      "440 3.155590954933021e-06\n",
      "441 2.9971891813773764e-06\n",
      "442 2.846796979647099e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443 2.7039554532291e-06\n",
      "444 2.568318567502596e-06\n",
      "445 2.4394625641650006e-06\n",
      "446 2.317128840683463e-06\n",
      "447 2.200929298383284e-06\n",
      "448 2.090573498666831e-06\n",
      "449 1.9857828100447913e-06\n",
      "450 1.8862599133831183e-06\n",
      "451 1.7917445102443659e-06\n",
      "452 1.701949454843139e-06\n",
      "453 1.6166906318374065e-06\n",
      "454 1.5357185818938653e-06\n",
      "455 1.4587980727387725e-06\n",
      "456 1.385749183168261e-06\n",
      "457 1.3163477122590708e-06\n",
      "458 1.25044264735104e-06\n",
      "459 1.1878747526932012e-06\n",
      "460 1.128436579973763e-06\n",
      "461 1.0720121016017676e-06\n",
      "462 1.0183795203003629e-06\n",
      "463 9.674248908053798e-07\n",
      "464 9.190347907690581e-07\n",
      "465 8.730824804061048e-07\n",
      "466 8.294355154074555e-07\n",
      "467 7.879794509117059e-07\n",
      "468 7.485836035567512e-07\n",
      "469 7.111715392250751e-07\n",
      "470 6.756266510321775e-07\n",
      "471 6.418676357416565e-07\n",
      "472 6.098019479560899e-07\n",
      "473 5.793464046254369e-07\n",
      "474 5.504135094356608e-07\n",
      "475 5.229252632337216e-07\n",
      "476 4.968137770319276e-07\n",
      "477 4.7201030375144606e-07\n",
      "478 4.484520211455931e-07\n",
      "479 4.260707641569098e-07\n",
      "480 4.0481724026102473e-07\n",
      "481 3.846157272442553e-07\n",
      "482 3.6542558722066255e-07\n",
      "483 3.472027532933069e-07\n",
      "484 3.2989540453175994e-07\n",
      "485 3.134543651388792e-07\n",
      "486 2.978237741483452e-07\n",
      "487 2.829772053405989e-07\n",
      "488 2.6887054589234896e-07\n",
      "489 2.55476147701833e-07\n",
      "490 2.4274503526663233e-07\n",
      "491 2.3064794952284698e-07\n",
      "492 2.1915729853055857e-07\n",
      "493 2.0823990452294013e-07\n",
      "494 1.9787110486495272e-07\n",
      "495 1.8802183746453664e-07\n",
      "496 1.786604718191213e-07\n",
      "497 1.6976578266390857e-07\n",
      "498 1.6131390658096078e-07\n",
      "499 1.5328474632804593e-07\n"
     ]
    }
   ],
   "source": [
    " #-*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 무작위로 가중치를 초기화\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 가중치를 갱신\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor를 사용한 신경망을 구성해보자.\n",
    "\n",
    "NumPy는 훌륭한 프레임워크지만, GPU를 사용하여 수치 연산을 가속화할 수는 없다. 현대의 심층 신경망에서 GPU는 종종 50배 또는 그 이상 의 속도 향상을 제공하기 때문에,  NumPy는 현대의 딥러닝에 적합하지 않다.\n",
    "\n",
    "PyTorch Tensor는 개념적으로 NumPy 배열과 동일하다. Tensor는 N차원 배열이며, PyTorch는 Tensor 연산을 위한 다양한 함수들을 제공한다. NumPy와는 달리, PyTorch Tensor는 GPU를 활용하여 수치 연산을 가속화할 수 있다. GPU에서 PyTorch Tensor를 실행하기 위해서는 단지 새로운 자료형으로 변환(Cast)해주기만 하면 된다.\n",
    "\n",
    "여기에서는 PyTorch Tensor를 사용하여 2계층의 신경망이 무작위 데이터를 맞추도록 할 것이다. 위의 NumPy 예제에서와 같이 신경망의 순전파 단계와 역전파 단계는 직접 구현한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 513.4201049804688\n",
      "199 2.68537974357605\n",
      "299 0.02145102247595787\n",
      "399 0.0003887308994308114\n",
      "499 5.218850856181234e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float # dtype = 데이터 타입\n",
    "device = torch.device(\"cpu\") # CPU에서 실행한다는 의미\n",
    "# device = torch.device(\"cuda:0\") # GPU에서 실행시 이 코드 사용\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 무작위로 가중치를 초기화\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd를 사용한 신경망을 구성해보자.\n",
    "\n",
    "위의 두 예제에서 신경망의 순전파 단계와 역전파 단계를 직접 구현하였다. 대규모의 복잡한 신경망에서는 이를 직접 구현하는것이 복잡하고 어렵다.\n",
    "\n",
    "다행히, Autograd를 사용하여 신경망에서 역전파 단계의 연산을 자동화할 수 있다. Autograd를 사용할 때, 신경망의 순전파 단계는 연산 그래프를 정의하게 된다. 이 그래프의 노드(node)는 Tensor가 되고, 엣지(edge)는 입력 Tensor로부터 출력 Tensor를 만들어내는 함수가 된다. 이 그래프를 통해 역전파를 하게 되면 변화도를 쉽게 계산할 수 있다.\n",
    "\n",
    "여기에서는 PyTorch Tensor와 autograd를 사용하여 2계층 신경망을 구현한다. 이제 신경망의 역전파 단계를 직접 구현할 필요가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 256.0006103515625\n",
      "199 0.6370574831962585\n",
      "299 0.0030558237340301275\n",
      "399 0.00010187493899138644\n",
      "499 2.1627420210279524e-05\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성\n",
    "# requires_grad=False로 설정하여 역전파 중에 이 Tensor들에 대한 변화도를 계산할\n",
    "# 필요가 없음을 나타낸다. (requres_grad의 기본값이 False이므로 아래 코드에는\n",
    "# 이를 반영하지 않았다.)\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성\n",
    "# requires_grad=True로 설정하여 역전파 중에 이 Tensor들에 대한\n",
    "# 변화도를 계산할 필요가 있음을 나타낸다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: Tensor 연산을 사용하여 예상되는 y 값을 계산\n",
    "    # 이는 Tensor를 사용한 순전파 단계와 완전히 동일하지만, 역전파 단계를 별도로\n",
    "    # 구현하지 않아도 되므로 중간값들에 대한 참조를 갖고 있을 필요가 없다.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Tensor 연산을 사용하여 손실을 계산하고 출력\n",
    "    # loss는 (1,) 형태의 Tensor이며, loss.item()은 loss의 스칼라 값\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograd를 사용하여 역전파 단계를 계산\n",
    "    # 이는 requires_grad=True를 갖는 모든 Tensor에 대해 손실의 변화도를 계산\n",
    "    # w1.grad와 w2.grad는 w1과 w2 각각에 대한 손실의 변화도를 갖는 Tensor가 된다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 수동으로 갱신\n",
    "    # torch.no_grad()로 감싸는 이유는 가중치들이 requires_grad=True이지만\n",
    "    # autograd에서는 이를 추적할 필요가 없기 때문이다.\n",
    "    # tensor.data가 tensor의 저장공간을 공유하지만, 이력을 추적하지 않는다는 것을 \n",
    "    # 기억하고, 이를 위해 torch.optim.SGD 를 사용할 수도 있습니다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 수동으로 변화도를 0으로 만든다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn패키지를 사용하여 신경망을 구성해보자.\n",
    "\n",
    "연산 그래프와 autograd는 복잡한 연산자를 정의하고 도함수(derivative)를 자동으로 계산하는 매우 강력한 패러다임이다. 하지만 규모가 큰 신경망에서는 autograd 그 자체만으로는 너무 낮은 수준(low-level)일 수 있다.\n",
    "\n",
    "신경망을 구성할 때 종종 연산을 여러 계층에 배열(arrange)하는 것으로 생각하는데, 이 중 일부는 학습 도중 최적화가 될 학습 가능한 매개변수를 갖고 있다.\n",
    "\n",
    "PyTorch에서 nn 패키지가 연산 그래프를 더 높은 수준으로 추상화(higher-level abstraction)하여 제공하므로 신경망을 구축하는데 있어 유용하다. nn 패키지는 신경망 계층(layer)들과 거의 동일한 Module의 집합을 정의한다. Module은 입력 Tensor를 받고 출력 Tensor를 계산하며, 학습 가능한 매개변수를 갖는 Tensor 같은 내부 상태(internal state)를 갖는다. 또 nn 패키지는 신경망을 학습시킬 때 주로 사용하는 유용한 손실 함수들도 정의하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.834005355834961\n",
      "199 0.03594784438610077\n",
      "299 0.0008381842053495347\n",
      "399 2.599265644676052e-05\n",
      "499 9.642888016969664e-07\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위한 Tensor 생성\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn 패키지를 사용하여 모델을 순차적 계층(sequence of layers)으로 정의\n",
    "# nn.Sequential은 다른 Module들을 포함하는 Module로, 그 Module들을 순차적으로\n",
    "# 적용하여 출력을 생성한다. 각각의 Linear Module은 선형 함수를 사용하여\n",
    "# 입력으로부터 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장한다.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산한다. Module 객체는\n",
    "    # __call__ 연산자를 덮어써(override) 함수처럼 호출할 수 있게 한다.\n",
    "    # 이렇게 함으로써 입력 데이터의 Tensor를 Module에 전달하여 출력 데이터의\n",
    "    # Tensor를 생성한다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력\n",
    "    # 예측한 y와 정답인 y를 갖는 Tensor들을 전달하고, 손실 값을 갖는 Tensor를 반환\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계를 실행하기 전에 변화도를 0으로 만든다.\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 모든 학습 가능한 매개변수의 변화도를 계산\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optim패키지를 사용하여 신경망을 구성해보자.\n",
    "\n",
    "지금까지는 (autograd의 추적 기록을 피하기 위해 torch.no_grad () 또는 .data를 사용하는 식으로) 학습 가능한 매개변수를 갖는 Tensor를 직접 조작하며 모델의 가중치를 갱신하였다. 이것은 확률적 경사 하강법(SGD)과 같은 간단한 최적화 알고리즘에서는 크게 부담이 되지 않지만, 실제로 신경망을 학습할 때는 주로 AdaGrad, RMSProp, Adam 등과 같은 좀 더 정교한 Optimizer를 사용한다.\n",
    "\n",
    "PyTorch의 optim 패키지는 최적화 알고리즘에 대한 아이디어를 추상화하고 일반적으로 사용하는 최적화 알고리즘의 구현체(implementation)를 제공한다.\n",
    "\n",
    "아래 예제에서는 nn 패키지를 사용하여 모델을 정의하고, optim 패키지가 제공하는 Adam 알고리즘을 이용하여 모델을 최적화한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 59.43168258666992\n",
      "199 1.1043533086776733\n",
      "299 0.007197318598628044\n",
      "399 3.527702938299626e-05\n",
      "499 7.885238062499411e-08\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn 패키지를 사용하여 모델과 손실 함수를 정의\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# optim 패키지를 사용하여 모델의 가중치를 갱신할 Optimizer를 정의\n",
    "# Adam을 사용, Adam 생성자의 첫번째 인자는 어떤 Tensor가 갱신되어야 하는지\n",
    "# 알려준다.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(500):\n",
    "    # 순전파 단계\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계 전에, Optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인)\n",
    "    # 갱신할 변수들에 대한 모든 변화도를 0으로 만든다. 이렇게 하는 이유는\n",
    "    # 기본적으로 .backward()를 호출할 때마다 변화도가 버퍼(buffer)에 (덮어쓰지 않고)\n",
    "    # 누적되기 때문이다.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 역전파 단계\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer의 step 함수를 호출하면 매개변수가 갱신된다.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module 클래스로 신경망을 구성해보자.\n",
    "\n",
    "기존 모듈의 구성(sequence)보다 더 복잡한 모델을 구성해야 할 때가 있다. 이럴 때는 nn.Module의 서브클래스로 새 모듈을 정의하고, 입력 Tensor를 받아 다른 모듈 또는 Tensor의 autograd 연산을 사용하여 출력 Tensor를 만드는 forward를 정의하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.141716480255127\n",
      "199 0.02532978728413582\n",
      "299 0.0005677043809555471\n",
      "399 1.9873172277584672e-05\n",
      "499 9.447001048101811e-07\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        생성자에서 2개의 nn.Linear 모듈을 생성하고, 멤버 변수로 지정\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 함수에서는 입력 데이터의 Tensor를 받고 출력 데이터의 Tensor를\n",
    "        반환해야 한다. Tensor 상의 임의의 연산자뿐만 아니라 생성자에서 정의한\n",
    "        Module도 사용할 수 있다.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 앞에서 정의한 클래스를 생성하여 모델을 구성\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# 손실 함수와 Optimizer를 생성한다. SGD 생성자에 model.parameters()를 호출하면\n",
    "# 모델의 멤버인 2개의 nn.Linear 모듈의 학습 가능한 매개변수들이 포함됩니다.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # 순전파 단계\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산 후 출력\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 변화도를 0으로 만들고, 역전파 단계를 수행 후 가중치 갱신\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제어 흐름(Control Flow) + 가중치 공유(Weight Sharing)가 되는 모델로 신경망을 구성해보자.\n",
    "\n",
    "각 순전파 단계에서 많은 은닉 계층을 갖는 완전히 연결(fully-connected)된 ReLU 신경망이 무작위로 0 ~ 3 사이의 숫자를 선택하고, 가장 안쪽(innermost)의 은닉층들을 계산하기 위해 동일한 가중치를 여러 번 재사용한다.\n",
    "\n",
    "이 모델에서는 일반적인 Python 제어 흐름을 사용하여 반복(loop)을 구현할 수 있으며, 순전파 단계를 정의할 때 단지 동일한 Module을 여러번 재사용함으로써 내부(innermost) 계층들 간의 가중치 공유를 구현할 수 있다.\n",
    "\n",
    "Module을 상속받는 서브클래스로 이를 구현해보겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 19.300527572631836\n",
      "199 18.16835594177246\n",
      "299 0.6223381757736206\n",
      "399 0.3568560779094696\n",
      "499 0.6921087503433228\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        생성자에서 순전파 단계에서 사용할 3개의 nn.Linear 인스턴스를 생성\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        모델의 순전파 단계에서, 무작위로 0, 1, 2 또는 3 중에 하나를 선택하고\n",
    "        은닉층을 계산하기 위해 여러번 사용한 middle_linear Module을 재사용한다.\n",
    "\n",
    "        각 순전파 단계는 동적 연산 그래프를 구성하기 때문에, 모델의 순전파 단계를\n",
    "        정의할 때 반복문이나 조건문과 같은 일반적인 Python 제어 흐름 연산자를 사용할\n",
    "        수 있다.\n",
    "\n",
    "        여기에서 연산 그래프를 정의할 때 동일 Module을 여러번 재사용하는 것이\n",
    "        완벽히 안전하다는 것을 알 수 있다. 이것이 각 Module을 한 번씩만 사용할\n",
    "        수 있었던 Lua Torch보다 크게 개선된 부분이다.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "    \n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 앞서 정의한 클래스를 생성(instantiating)하여 모델을 구성\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# 손실함수와 Optimizer를 생성한다. 이 모델은 순수한 확률적 경사 하강법\n",
    "# (stochastic gradient decent)으로 학습하는 것은 어려우므로, 모멘텀(momentum)을\n",
    "# 사용한다.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "for t in range(500):\n",
    "    # 순전파 단계\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산 후 출력\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 변화도를 0으로 만들고, 역전파 단계를 수행한 후 가중치 갱신\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch37",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
